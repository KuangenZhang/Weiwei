# -*- coding: utf-8 -*-
"""Class_Regr_with_expdata0325.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbtVRL2NmrDvP39bt2hBs_PtyZ7XNuX6
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/My Drive/Colab Notebooks/

# dataset explanation 
# Data_PCF_1.npy: the first one i give you 
# Data_PCF_with_reacted_control_removed.npy: has some new data 
# Data_PC_with_reacted_control_removed:: has new data but without the 3rd class

from __future__ import print_function
import torch.utils.data as data
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from torchvision import datasets, transforms
from torch.autograd import Variable
from sklearn.model_selection import train_test_split
from tqdm import tqdm
from sklearn.metrics import confusion_matrix
from torchsummary import summary
from matplotlib import pyplot as plt
import matplotlib as mpl
mpl.rcParams['figure.dpi']= 150

device_name = "cuda" if torch.cuda.is_available() else "cpu"
device = torch.device(device_name)

# organize both ros label and class info 
class Dataset(data.Dataset):
    """Args:
        transform (callable, optional): A function/transform that  takes in an PIL image
            and returns a transformed version. E.g, ``transforms.RandomCrop``
        target_transform (callable, optional): A function/transform that takes in the
            target and transforms it.
    """
    def __init__(self, data, label, classes,
                 transform=None,target_transform=None):
        self.transform = transform
        self.target_transform = target_transform
        self.data = data
        self.labels = label
        self.classes = classes

    def __getitem__(self, index):
        """
         Args:
             index (int): Index
         Returns:
             tuple: (image, target) where target is index of the target class.
         """
        img, ros_target, class_target = self.data[index], self.labels[index], self.classes[index]

        img = np.float32(img)
        ros_target = np.float32(ros_target)

        return img, ros_target, class_target 
    def __len__(self):
        return len(self.data)

# for regression 
class regression_Net(nn.Module):
    def __init__(self, output_num=1, hidden_size = 100):
        super(regression_Net, self).__init__()
        self.hidden_size = hidden_size
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=hidden_size, kernel_size=3, stride=1, padding=0, bias=False)
        self.conv1_bn = nn.BatchNorm2d(hidden_size)
        self.conv2 = nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3, stride=1, padding=0,
                               bias=False)
        self.conv2_bn = nn.BatchNorm2d(hidden_size)
        self.conv3 = nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3, stride=1, padding=0,
                               bias=False)
        self.conv3_bn = nn.BatchNorm2d(hidden_size)
        # self.conv4 = nn.Conv2d(in_channels=hidden_size, out_channels=hidden_size, kernel_size=3, stride=1, padding=0,
        #                        bias=False)
        # self.conv4_bn = nn.BatchNorm2d(hidden_size)
        
        self.fc1 = nn.Linear(in_features=13 * 13 * hidden_size, out_features=hidden_size)
        self.fc1_bn = nn.BatchNorm1d(hidden_size)
        self.fc2 = nn.Linear(in_features=hidden_size, out_features=hidden_size)
        self.fc2_bn = nn.BatchNorm1d(hidden_size)
        self.fc3 = nn.Linear(in_features=hidden_size, out_features=hidden_size)
        self.fc4 = nn.Linear(in_features=hidden_size, out_features=hidden_size)
        self.fc5 = nn.Linear(in_features=hidden_size, out_features=hidden_size)
        self.fc6 = nn.Linear(in_features=hidden_size, out_features=output_num)
        self.dropout = nn.Dropout2d(p=0.3)

    def forward(self, x):
        # x [1, 28, 28]
        x = x.view((-1, 1, 60, 60))
        # x = F.relu(self.conv1_bn(self.conv1(x)))  # [58, 58]
        x = F.relu(self.conv1(x))  # [58, 58]
        # x = self.dropout(x)
        x = F.max_pool2d(x, 2, 2)  # [29, 29]
        # x = F.relu(self.conv2_bn(self.conv2(x)))  # [27, 27]
        x = F.relu(self.conv2(x))  # [27, 27]
        # x = self.dropout(x)
        x = F.max_pool2d(x, 2, 2)  # [13, 13]
        # x = F.relu(self.conv3_bn(self.conv3(x)))  # [11, 11]
        # x = F.relu(self.conv3(x))  # [11, 11]
        # # x = self.dropout(x)
        # x = F.max_pool2d(x, 2, 2)  # [5, 5]
        # x = F.relu(self.conv4_bn(self.conv4(x)))  # [3, 3]
        # x = F.relu(self.conv4(x))  # [3, 3]
        # x = self.dropout(x)
        # x = F.max_pool2d(x, 2, 2)  # [1, 1]

        x = x.view(-1, 13 * 13 * 100)
        # x = F.relu(self.fc1_bn(self.fc1(x)))
        x = F.relu(self.fc1(x))
        # x = self.dropout(x)
        # x = F.relu(self.fc2_bn(self.fc2(x)))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x))

        x = self.fc6(x)
        return x

# for regression 
def train_rg(model, train_loader, optimizer, epoch):
    model.train()
    for data, label, class_label in train_loader:
        # print(label.shape)
        optimizer.zero_grad()
        data = Variable(data.float().to(device))
        label = Variable((label).float().to(device))
        pred = model(data)
        loss = F.mse_loss(pred, label)
        loss.backward()
        optimizer.step()


def test_rg(model, test_loader):
    model.eval()
    y_list = []
    y_pred_list = []
    class_label_list = []
    with torch.no_grad():  # validation does not require gradient
        for X, y, class_label in test_loader:
            X = Variable(torch.FloatTensor(X)).to(device)
            # print(X.size(), y.size(), class_label.size())
            y = Variable(torch.FloatTensor(y)).to(device)
            y_pred = model(X)
            # y_pred = y_pred.argmax(dim=1, keepdim=True)
            y_list.append(y)
            y_pred_list.append(y_pred)
            class_label_list.append(class_label)

    y = torch.cat(y_list, dim=0).cpu().detach().numpy()
    y_pred = torch.cat(y_pred_list, dim=0).cpu().detach().numpy()
    class_label = torch.cat(class_label_list, dim=0).cpu().detach().numpy()
    mse_loss = np.mean(np.square(y - y_pred))
    return mse_loss, y, y_pred, class_label

# new split function, take normalization into account
def split_data_norm (batch_size, phase_norm, ros_norm):
    data = np.load('Data_PCF_with_reacted_control_removed.npy', allow_pickle=True).item()
    phase_img = data['Images']
    ros_level = data['ros_level']
    condition = data['classes']

    phase_img = phase_img[condition<2]
    ros_level = ros_level[condition < 2]
    condition = condition[condition < 2]

    if phase_norm == True:
        phase_img = phase_img / np.amax(phase_img)
        print('img_data: {}, {}'.format(np.amax(phase_img), np.amin(phase_img)))

    if ros_norm == True:
        rmin = np.amin(ros_level)
        r_mean = np.mean(ros_level)
        r_std = np.std(ros_level)
        ros_level= np.tanh((0.1 * (ros_level - r_mean) / r_std ) - (0.1 * (rmin - r_mean) / r_std))
        print('ros_data: {}, {}'.format(np.amax(ros_level), np.amin(ros_level)))

    ros_level = np.expand_dims(ros_level, axis=-1)

    dataset = Dataset(phase_img, ros_level, condition)

    train_size = int(0.8 * len(condition))
    test_size = len(condition) - train_size 
    print('Train size: {:d}% Test size: {:d}%'.format(train_size, test_size))

    

    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size],
                                                                            generator=torch.Generator().manual_seed(42))

    data_loader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    data_loader_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)
    return data_loader_train, data_loader_test, train_dataset, test_dataset

def visualization(y, y_pred, condition):
    # visiliza based on class labels
    control_index = [c for c in range(len(condition)) if condition[c] == 0]
    pma_index = [c for c in range(len(condition)) if condition[c] == 1]
    fmlp_index = [c for c in range(len(condition)) if condition[c] == 2]

    fig, ax = plt.subplots(1, 2, figsize=(10, 5), sharey='col')
    ax_f = ax.flatten()
    for ax in ax.ravel():  # ravel axes to a flattened array
        ax.set_ylim(0, 6)

    y_c = y[control_index]
    y_c_pre = y_pred[control_index]
    ax_f[0].scatter(range(len(y_c)), y_c, s=5, c='slategrey')
    ax_f[0].scatter(range(len(y_c)), y_c_pre, s=5, c='indianred')

    y_p = y[pma_index]
    y_p_pre = y_pred[pma_index]
    ax_f[1].scatter(range(len(y_p)), y_p, s=5, c='slategrey')
    ax_f[1].scatter(range(len(y_p)), y_p_pre, s=5, c='indianred', alpha=0.5)

    # y_f = y[fmlp_index]
    # y_f_pre = y_pred[fmlp_index]
    # ax_f[2].scatter(range(len(y_f)), y_f, s=5, c='slategrey')
    # ax_f[2].scatter(range(len(y_f)), y_f_pre, s=5, c='indianred')
    plt.savefig('image/result.pdf', bbox_inches='tight')
    plt.savefig('image/result.png', bbox_inches='tight')
    plt.show()


def reverse_to_or_ros(y, ros_std, rmin):
    y_inter = 0.5 * np.log((y + 1) / (1 - y))
    y_or = y_inter * ros_std / 0.1 + rmin
    return y_or

def main_rg():
    lr = 1e-3
    epochs = 50
    train_loader, test_loader, train_dataset, test_dataset = split_data_norm(32, False, False)
    model = regression_Net().to(device)
    summary(model, (1, 60, 60))

    is_train = False
    if is_train:
        optimizer = optim.Adam(model.parameters(), lr=lr)
        best_mse_test = 1e3
        for epoch in tqdm(range(epochs)):
            train_rg(model, train_loader, optimizer, epoch)
            mse_train, _, _, _ = test_rg(model, train_loader)
            mse_test, _, _, _ = test_rg(model, test_loader)
            print('Training loss : {:.4f}, test loss {:.4f}'.format(mse_train, mse_test))
            torch.save(model.state_dict(), "no_phase_ros_norm.pt")
            if mse_test < best_mse_test:
                best_mse_test = mse_test

    model.load_state_dict(torch.load("no_phase_ros_norm.pt"))
    mse_test, y, y_pred, class_label = test_rg(model, test_loader)
    print('Final test accuracy: {:.2f}%'.format(mse_test))
    # plot_comfusion_matrix(y, y_pred)
    visualization(y, y_pred, class_label)






#data visulization 
# high_value = [m for m in range(len(y_pre)) if y_test[m] >=1.5]
# y_1 = y_test[high_value]
# y_2 = y_pre[high_value]
# plt.figure()
# plt.scatter(range(len(y)), y, s=5, c='slategrey')
# plt.scatter(range(len(y)), y_pred, s=5, c='indianred')
# y_or = reverse_to_or_ros (y, ros_std, rmin)
# y_pred_or = reverse_to_or_ros (y_pred, ros_std, rmin)

# plt.figure(figsize= (20, 8))
# fig, ax = plt.subplots(1, 2)
# ax_f = ax.flatten()
# ax_f[0].scatter(range(len(y)), y, s=5, c='slategrey')
# ax_f[0].scatter(range(len(y)), y_pred, s=5, c='indianred')

# ax_f[1].scatter(range(len(y)), y_or, s=5, c='slategrey')
# ax_f[1].scatter(range(len(y)), y_pred_or, s=5, c='indianred')

if __name__ == '__main__':
    main_rg()